#!/usr/bin/env python3
"""
DID/VCãƒ»AI ãƒ‹ãƒ¥ãƒ¼ã‚¹æ—¥æ¬¡åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ã€æƒ…å ±åé›†ã®ä»•çµ„ã¿ã€‘

1. Google News RSS + å¤–éƒ¨API (Hacker Newsç­‰) ã‹ã‚‰è¨˜äº‹ã‚’åé›†  (Python)
2. å…¬é–‹æ—¥ã§å³å¯†ã«ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆéå»24æ™‚é–“ä»¥å†…ã®ã¿ï¼‰            (Python)
3. ã‚¿ã‚¤ãƒˆãƒ«é¡ä¼¼åº¦ã§é‡è¤‡æ’é™¤                                (Python)
4. Claude Code ã§é–¢é€£æ€§åˆ¤å®š + è¦ç´„                         (claude -p)
5. Markdown ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ                                   (Python)

URLãƒ»æ—¥ä»˜ãƒ»ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¯ã™ã¹ã¦ Python ãŒä¿è¨¼ã™ã‚‹ã€‚
Claude ã¯é–¢é€£æ€§åˆ¤å®šã¨è¦ç´„ã®ã¿ã‚’æ‹…å½“ã—ã€å†ç¾æ€§ã‚’ç¢ºä¿ã™ã‚‹ã€‚

ä½¿ã„æ–¹:
  python3 collect.py did-vc
  python3 collect.py all
  python3 collect.py ai-dev --date 2026-02-18
"""

import json
import os
import re
import subprocess
import sys
import urllib.parse
import urllib.request
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta, timezone

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
CONFIG_FILE = os.environ.get("DAILY_NEWS_CONFIG") or os.path.join(SCRIPT_DIR, "config.json")
PROMPT_TEMPLATE_FILE = os.path.join(SCRIPT_DIR, "prompt_template.md")
REPORTS_DIR = os.environ.get("DAILY_NEWS_REPORTS_DIR") or os.path.join(SCRIPT_DIR, "reports")

JST = timezone(timedelta(hours=9))


# =============================================================================
# è¨˜äº‹åé›†
# =============================================================================

def fetch_google_news_rss(keyword: str, lang: str, since: datetime) -> list[dict]:
    """Google News RSS ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—ã™ã‚‹ã€‚"""
    encoded = urllib.parse.quote(keyword)
    if lang == "ja":
        url = f"https://news.google.com/rss/search?q={encoded}+when:1d&hl=ja&gl=JP&ceid=JP:ja"
    else:
        url = f"https://news.google.com/rss/search?q={encoded}+when:1d&hl=en&gl=US&ceid=US:en"

    articles = []
    try:
        req = urllib.request.Request(url, headers={"User-Agent": "daily-news-collector/1.0"})
        with urllib.request.urlopen(req, timeout=15) as resp:
            xml_data = resp.read()
        root = ET.fromstring(xml_data)

        for item in root.findall(".//item"):
            title = item.findtext("title", "")
            link = item.findtext("link", "")
            pub_date_str = item.findtext("pubDate", "")
            source_el = item.find("source")
            source = source_el.text if source_el is not None else ""

            pub_date = parse_rss_date(pub_date_str)
            if pub_date is None or pub_date < since:
                continue

            articles.append({
                "title": title,
                "url": link,
                "published": pub_date.isoformat(),
                "source": source,
                "lang": lang,
                "via": "google_news",
            })
    except Exception as e:
        print(f"  [WARN] Google News RSS failed for '{keyword}': {e}", file=sys.stderr)

    return articles


def fetch_hackernews(queries: list[str], since_ts: int) -> list[dict]:
    """Hacker News Algolia API ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—ã™ã‚‹ã€‚"""
    articles = []
    seen_urls = set()

    for query in queries:
        url = (
            "https://hn.algolia.com/api/v1/search_by_date"
            f"?query={urllib.parse.quote(query)}"
            f"&tags=story"
            f"&numericFilters=created_at_i>{since_ts}"
            f"&hitsPerPage=10"
        )
        try:
            req = urllib.request.Request(url, headers={"User-Agent": "daily-news-collector/1.0"})
            with urllib.request.urlopen(req, timeout=10) as resp:
                data = json.loads(resp.read())
        except Exception as e:
            print(f"  [WARN] HN query '{query}' failed: {e}", file=sys.stderr)
            continue

        for hit in data.get("hits", []):
            article_url = hit.get("url") or f"https://news.ycombinator.com/item?id={hit['objectID']}"
            if article_url in seen_urls:
                continue
            seen_urls.add(article_url)

            articles.append({
                "title": hit.get("title", ""),
                "url": article_url,
                "published": hit.get("created_at", ""),
                "source": "Hacker News",
                "lang": "en",
                "via": "hackernews",
                "hn_url": f"https://news.ycombinator.com/item?id={hit['objectID']}",
                "points": hit.get("points", 0),
            })

    return articles


def parse_rss_date(date_str: str) -> datetime | None:
    """RSS ã®æ—¥ä»˜æ–‡å­—åˆ—ã‚’ãƒ‘ãƒ¼ã‚¹ã™ã‚‹ã€‚"""
    formats = [
        "%a, %d %b %Y %H:%M:%S %z",
        "%a, %d %b %Y %H:%M:%S GMT",
        "%Y-%m-%dT%H:%M:%S%z",
        "%Y-%m-%dT%H:%M:%SZ",
    ]
    for fmt in formats:
        try:
            dt = datetime.strptime(date_str.strip(), fmt)
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=timezone.utc)
            return dt
        except ValueError:
            continue
    return None


# =============================================================================
# ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ»é‡è¤‡æ’é™¤
# =============================================================================

def filter_by_date(articles: list[dict], since: datetime) -> list[dict]:
    """å…¬é–‹æ—¥ãŒ since ä»¥é™ã®è¨˜äº‹ã®ã¿æ®‹ã™ã€‚"""
    result = []
    for a in articles:
        try:
            pub = datetime.fromisoformat(a["published"])
            if pub.tzinfo is None:
                pub = pub.replace(tzinfo=timezone.utc)
            if pub >= since:
                result.append(a)
        except (ValueError, KeyError):
            continue
    return result


def deduplicate(articles: list[dict]) -> list[dict]:
    """ã‚¿ã‚¤ãƒˆãƒ«ã®å˜èªé›†åˆã® Jaccard ä¿‚æ•°ã§é‡è¤‡æ’é™¤ï¼ˆé–¾å€¤ 0.6ï¼‰ã€‚"""
    def tokenize(title: str) -> set[str]:
        return set(re.findall(r'\w+', title.lower()))

    result = []
    seen_tokens = []
    for a in articles:
        tokens = tokenize(a["title"])
        is_dup = False
        for seen in seen_tokens:
            intersection = tokens & seen
            union = tokens | seen
            if union and len(intersection) / len(union) > 0.6:
                is_dup = True
                break
        if not is_dup:
            result.append(a)
            seen_tokens.append(tokens)
    return result


def filter_excluded_keywords(articles: list[dict], exclude: list[str]) -> list[dict]:
    """é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€è¨˜äº‹ã‚’é™¤å¤–ã€‚"""
    if not exclude:
        return articles
    result = []
    for a in articles:
        title_lower = a["title"].lower()
        if not any(kw.lower() in title_lower for kw in exclude):
            result.append(a)
    return result


# =============================================================================
# Claude ã§é–¢é€£æ€§åˆ¤å®š + è¦ç´„
# =============================================================================

def call_claude(articles: list[dict], topic_label: str) -> dict | None:
    """claude -p ã§è¨˜äº‹ã®é–¢é€£æ€§åˆ¤å®šã¨è¦ç´„ã‚’è¡Œã†ã€‚"""
    with open(PROMPT_TEMPLATE_FILE) as f:
        template = f.read()

    # è¨˜äº‹ã« ID ã‚’ä»˜ä¸
    for i, a in enumerate(articles):
        a["id"] = str(i)

    articles_json = json.dumps(articles, ensure_ascii=False, indent=2)
    prompt = template.replace("{{TOPIC_LABEL}}", topic_label)
    prompt = prompt.replace("{{ARTICLES_JSON}}", articles_json)

    try:
        env = os.environ.copy()
        env.pop("CLAUDECODE", None)  # ãƒã‚¹ãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³æ¤œå‡ºã‚’å›é¿
        result = subprocess.run(
            ["claude", "-p"],
            input=prompt,
            capture_output=True,
            text=True,
            timeout=120,
            env=env,
        )
        output = result.stdout.strip()
    except subprocess.TimeoutExpired:
        print("  [ERROR] claude -p timed out", file=sys.stderr)
        return None
    except FileNotFoundError:
        print("  [ERROR] claude command not found", file=sys.stderr)
        return None

    return _extract_json(output, "CLI")


def _extract_json(output: str, source: str) -> dict | None:
    """å‡ºåŠ›æ–‡å­—åˆ—ã‹ã‚‰ JSON ã‚’æŠ½å‡ºã™ã‚‹ã€‚ã‚³ãƒ¼ãƒ‰ãƒ•ã‚§ãƒ³ã‚¹ãŒã‚ã‚Œã°é™¤å»ã™ã‚‹ã€‚"""
    # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã®ã‚³ãƒ¼ãƒ‰ãƒ•ã‚§ãƒ³ã‚¹ã‚’é™¤å»
    cleaned = re.sub(r'```(?:json)?\s*', '', output)
    cleaned = cleaned.strip()

    json_match = re.search(r'\{[\s\S]*\}', cleaned)
    if json_match:
        try:
            return json.loads(json_match.group())
        except json.JSONDecodeError:
            print(f"  [WARN] Failed to parse {source} output as JSON", file=sys.stderr)
            print(f"  Output: {output[:500]}", file=sys.stderr)
            return None
    else:
        print(f"  [WARN] No JSON found in {source} output", file=sys.stderr)
        print(f"  Output: {output[:500]}", file=sys.stderr)
        return None


def call_claude_api(articles: list[dict], topic_label: str, model: str) -> dict | None:
    """Anthropic Messages API ã§è¨˜äº‹ã®é–¢é€£æ€§åˆ¤å®šã¨è¦ç´„ã‚’è¡Œã†ã€‚"""
    from anthropic import Anthropic

    with open(PROMPT_TEMPLATE_FILE) as f:
        template = f.read()

    for i, a in enumerate(articles):
        a["id"] = str(i)

    articles_json = json.dumps(articles, ensure_ascii=False, indent=2)
    prompt = template.replace("{{TOPIC_LABEL}}", topic_label)
    prompt = prompt.replace("{{ARTICLES_JSON}}", articles_json)

    try:
        client = Anthropic()
        with client.messages.stream(
            model=model,
            max_tokens=32768,
            messages=[{"role": "user", "content": prompt}],
        ) as stream:
            message = stream.get_final_message()
        output = message.content[0].text.strip()
        if message.stop_reason == "max_tokens":
            print(f"  [WARN] API response truncated (max_tokens=32768, articles={len(articles)})", file=sys.stderr)
    except Exception as e:
        print(f"  [ERROR] Anthropic API call failed: {e}", file=sys.stderr)
        return None

    return _extract_json(output, "API")


def analyze_with_claude(articles: list[dict], topic_label: str) -> dict | None:
    """ANTHROPIC_API_KEY ã®æœ‰ç„¡ã§ API ãƒ¢ãƒ¼ãƒ‰ã¨ CLI ãƒ¢ãƒ¼ãƒ‰ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹ã€‚"""
    api_key = os.environ.get("ANTHROPIC_API_KEY")
    model = os.environ.get("CLAUDE_MODEL", "claude-sonnet-4-6")
    if api_key:
        print(f"  [MODE] API ãƒ¢ãƒ¼ãƒ‰ (model={model})", file=sys.stderr)
        return call_claude_api(articles, topic_label, model)
    else:
        print("  [MODE] CLI ãƒ¢ãƒ¼ãƒ‰ (claude -p)", file=sys.stderr)
        return call_claude(articles, topic_label)


# =============================================================================
# ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
# =============================================================================

def merge_results(
    articles: list[dict],
    claude_result: dict | None,
) -> tuple[list[dict], list[dict]]:
    """Claude ã®çµæœã¨å…ƒè¨˜äº‹ã‚’ãƒãƒ¼ã‚¸ã—ã€(selected, excluded) ã‚’è¿”ã™ã€‚"""
    excluded = []
    if claude_result and claude_result.get("articles"):
        rated = {a["id"]: a for a in claude_result["articles"]}
        selected = []
        for a in articles:
            if a["id"] in rated:
                a["summary_ja"] = rated[a["id"]].get("summary_ja", "")
                a["importance"] = rated[a["id"]].get("importance", "medium")
                selected.append(a)
            else:
                excluded.append(a)
    else:
        selected = articles
        for a in selected:
            a["summary_ja"] = ""
            a["importance"] = None

    importance_order = {"high": 0, "medium": 1, "low": 2}
    selected.sort(key=lambda a: importance_order.get(a.get("importance"), 3))
    return selected, excluded


def generate_report_json(
    topic_id: str,
    topic_label: str,
    selected: list[dict],
    excluded: list[dict],
    claude_result: dict | None,
    date_str: str,
    total_fetched: int,
    total_after_filter: int,
) -> dict:
    """æ§‹é€ åŒ–ã•ã‚ŒãŸ JSON ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã€‚"""
    def clean_article(a: dict) -> dict:
        return {
            "title": a.get("title", ""),
            "url": a.get("url", ""),
            "published": a.get("published", ""),
            "source": a.get("source", ""),
            "lang": a.get("lang", ""),
            "summary_ja": a.get("summary_ja", ""),
            "importance": a.get("importance"),
            "hn_url": a.get("hn_url"),
        }

    en_articles = [clean_article(a) for a in selected if a.get("lang") == "en"]
    ja_articles = [clean_article(a) for a in selected if a.get("lang") == "ja"]

    return {
        "topic": topic_id,
        "label": topic_label,
        "date": date_str,
        "collected_at": datetime.now(JST).strftime("%Y-%m-%d %H:%M"),
        "highlights": claude_result.get("highlights", "") if claude_result else "",
        "articles_en": en_articles,
        "articles_ja": ja_articles,
        "meta": {
            "total_fetched": total_fetched,
            "after_filter": total_after_filter,
            "selected_en": len(en_articles),
            "selected_ja": len(ja_articles),
            "excluded": len(excluded),
            "excluded_reasons": claude_result.get("excluded_reasons", "") if claude_result else "",
        },
    }


def generate_report(
    topic_label: str,
    selected: list[dict],
    excluded: list[dict],
    claude_result: dict | None,
    date_str: str,
    total_fetched: int,
    total_after_filter: int,
) -> str:
    """Markdown ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã€‚"""
    now = datetime.now(JST).strftime("%Y-%m-%d %H:%M")

    # è‹±èªãƒ»æ—¥æœ¬èªã«åˆ†å‰²
    en_articles = [a for a in selected if a.get("lang") == "en"]
    ja_articles = [a for a in selected if a.get("lang") == "ja"]

    lines = [
        f"# {topic_label} - {date_str}",
        "",
        f"> åé›†æ™‚åˆ»: {now} JST",
        "> å¯¾è±¡æœŸé–“: éå»24æ™‚é–“",
        "",
        f"- [æ—¥æœ¬èªãƒ‹ãƒ¥ãƒ¼ã‚¹ ({len(ja_articles)}ä»¶)](#æ—¥æœ¬èªãƒ‹ãƒ¥ãƒ¼ã‚¹)",
        f"- [English News ({len(en_articles)}ä»¶)](#english-news)",
        "",
    ]

    # æ—¥æœ¬èªãƒ‹ãƒ¥ãƒ¼ã‚¹
    lines.append("## æ—¥æœ¬èªãƒ‹ãƒ¥ãƒ¼ã‚¹")
    lines.append("")
    if ja_articles:
        for i, a in enumerate(ja_articles, 1):
            importance = a.get("importance")
            lines.append(f"### {i}. [{a['title']}]({a['url']})")
            if importance:
                importance_badge = {"high": "ğŸ”´ HIGH", "medium": "ğŸŸ¡ MEDIUM", "low": "ğŸ”µ LOW"}.get(importance, importance)
                lines.append(f"- **é‡è¦åº¦**: {importance_badge}")
            lines.append(f"- **å…¬é–‹æ—¥**: {a['published']}")
            lines.append(f"- **Source**: {a['source']}")
            if a.get("summary_ja"):
                lines.append(f"- **è¦ç´„**: {a['summary_ja']}")
            lines.append("")
    else:
        lines.append("è©²å½“ãªã—")
        lines.append("")

    # English News
    lines.append("## English News")
    lines.append("")
    if en_articles:
        for i, a in enumerate(en_articles, 1):
            importance = a.get("importance")
            lines.append(f"### {i}. [{a['title']}]({a['url']})")
            if importance:
                importance_badge = {"high": "ğŸ”´ HIGH", "medium": "ğŸŸ¡ MEDIUM", "low": "ğŸ”µ LOW"}.get(importance, importance)
                lines.append(f"- **Importance**: {importance_badge}")
            lines.append(f"- **Published**: {a['published']}")
            lines.append(f"- **Source**: {a['source']}")
            if a.get("hn_url"):
                lines.append(f"- **HN**: {a['hn_url']}")
            if a.get("summary_ja"):
                lines.append(f"- **Summary**: {a['summary_ja']}")
            lines.append("")
    else:
        lines.append("è©²å½“ãªã—")
        lines.append("")

    # ãƒã‚¤ãƒ©ã‚¤ãƒˆ
    highlights = ""
    if claude_result:
        highlights = claude_result.get("highlights", "")
    if highlights:
        lines.append("## æœ¬æ—¥ã®ãƒã‚¤ãƒ©ã‚¤ãƒˆ")
        lines.append("")
        lines.append(highlights)
        lines.append("")

    # é™¤å¤–è¨˜äº‹ä¸€è¦§
    if excluded:
        # Claude ãŒè¿”ã—ãŸæ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«ã‚’ãƒãƒ¼ã‚¸
        excluded_ja = {}
        if claude_result and claude_result.get("excluded"):
            for e in claude_result["excluded"]:
                excluded_ja[e["id"]] = e.get("title_ja", "")

        lines.append("## é™¤å¤–ã•ã‚ŒãŸè¨˜äº‹")
        lines.append("")
        for a in excluded:
            title = excluded_ja.get(a["id"], "") or a["title"]
            lines.append(f"- {title}")
            lines.append(f"  {a['url']}")
        lines.append("")

    lines.append("## åé›†ãƒ¡ã‚¿æƒ…å ±")
    lines.append(f"- è¨˜äº‹å–å¾—æ•°ï¼ˆãƒ•ã‚£ãƒ«ã‚¿å‰ï¼‰: {total_fetched}")
    lines.append(f"- æ—¥ä»˜ãƒ»é‡è¤‡ãƒ•ã‚£ãƒ«ã‚¿å¾Œ: {total_after_filter}")
    lines.append(f"- æœ€çµ‚æ²è¼‰è¨˜äº‹æ•°: è‹±èª {len(en_articles)}ä»¶ / æ—¥æœ¬èª {len(ja_articles)}ä»¶")
    lines.append(f"- AIé™¤å¤–è¨˜äº‹æ•°: {len(excluded)}ä»¶")
    excluded_reasons = claude_result.get("excluded_reasons", "") if claude_result else ""
    if excluded_reasons:
        lines.append(f"- é™¤å¤–ç†ç”±: {excluded_reasons}")
    lines.append("")

    return "\n".join(lines)


# =============================================================================
# ãƒ¡ã‚¤ãƒ³
# =============================================================================

def collect_topic(topic_id: str, topic_config: dict, date_str: str) -> tuple[str, str]:
    """1ã¤ã®ãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦åé›†ãƒ»åˆ†æãƒ»ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚’è¡Œã†ã€‚(md_path, json_path) ã‚’è¿”ã™ã€‚"""
    label = topic_config["label"]
    print(f"--- {topic_id} åé›†é–‹å§‹ ---", file=sys.stderr)

    now = datetime.now(timezone.utc)
    since = now - timedelta(hours=24)
    since_ts = int(since.timestamp())

    all_articles = []

    # Google News RSSï¼ˆè‹±èªï¼‰
    print("  Google News (EN)...", file=sys.stderr)
    for kw in topic_config.get("keywords_en", []):
        arts = fetch_google_news_rss(kw, "en", since)
        all_articles.extend(arts)
        print(f"    '{kw}' â†’ {len(arts)}ä»¶", file=sys.stderr)

    # Google News RSSï¼ˆæ—¥æœ¬èªï¼‰
    print("  Google News (JA)...", file=sys.stderr)
    for kw in topic_config.get("keywords_ja", []):
        arts = fetch_google_news_rss(kw, "ja", since)
        all_articles.extend(arts)
        print(f"    '{kw}' â†’ {len(arts)}ä»¶", file=sys.stderr)

    # å¤–éƒ¨API
    for api in topic_config.get("apis", []):
        if api["type"] == "hackernews":
            print("  Hacker News...", file=sys.stderr)
            arts = fetch_hackernews(api["queries"], since_ts)
            all_articles.extend(arts)
            print(f"    â†’ {len(arts)}ä»¶", file=sys.stderr)

    total_fetched = len(all_articles)
    print(f"  åˆè¨ˆå–å¾—: {total_fetched}ä»¶", file=sys.stderr)

    # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆPython ã§å³å¯†ã«ï¼‰
    all_articles = filter_by_date(all_articles, since)

    # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    all_articles = filter_excluded_keywords(all_articles, topic_config.get("exclude_keywords", []))

    # é‡è¤‡æ’é™¤
    all_articles = deduplicate(all_articles)
    total_after_filter = len(all_articles)
    print(f"  ãƒ•ã‚£ãƒ«ã‚¿å¾Œ: {total_after_filter}ä»¶", file=sys.stderr)

    # Claude ã§é–¢é€£æ€§åˆ¤å®š + è¦ç´„
    claude_result = None
    if all_articles:
        print("  Claude ã§åˆ†æä¸­...", file=sys.stderr)
        claude_result = analyze_with_claude(all_articles, label)
        if claude_result:
            print(f"  â†’ {len(claude_result.get('articles', []))}ä»¶ã‚’æ¡ç”¨", file=sys.stderr)
        else:
            print("  â†’ Claude åˆ†æå¤±æ•—ï¼ˆè¨˜äº‹ã¯è¦ç´„ãªã—ã§æ²è¼‰ï¼‰", file=sys.stderr)

    # ãƒãƒ¼ã‚¸
    selected, excluded = merge_results(all_articles, claude_result)

    # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
    output_dir = os.path.join(REPORTS_DIR, topic_id)
    os.makedirs(output_dir, exist_ok=True)

    # Markdown
    report = generate_report(label, selected, excluded, claude_result, date_str, total_fetched, total_after_filter)
    md_file = os.path.join(output_dir, f"{date_str}.md")
    with open(md_file, "w") as f:
        f.write(report)

    # JSON
    report_json = generate_report_json(
        topic_id, label, selected, excluded, claude_result, date_str, total_fetched, total_after_filter
    )
    json_file = os.path.join(output_dir, f"{date_str}.json")
    with open(json_file, "w") as f:
        json.dump(report_json, f, ensure_ascii=False, indent=2)

    print(f"--- {topic_id} åé›†å®Œäº†: {md_file} / {json_file} ---", file=sys.stderr)
    return md_file, json_file


def main():
    import argparse

    parser = argparse.ArgumentParser(description="ãƒ‹ãƒ¥ãƒ¼ã‚¹æ—¥æ¬¡åé›†")
    parser.add_argument("topic", help="ãƒˆãƒ”ãƒƒã‚¯å or 'all'ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§è¤‡æ•°æŒ‡å®šå¯ï¼‰")
    parser.add_argument("--date", default=datetime.now(JST).strftime("%Y-%m-%d"), help="ãƒ¬ãƒãƒ¼ãƒˆã®æ—¥ä»˜")
    args = parser.parse_args()

    with open(CONFIG_FILE) as f:
        config = json.load(f)

    topics = config["topics"]
    valid_topics = list(topics.keys())

    # å®Ÿè¡Œã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯ã‚’æ±ºå®šï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šå¯¾å¿œï¼‰
    if args.topic == "all":
        target_topics = valid_topics
    else:
        target_topics = [t.strip() for t in args.topic.split(",")]
        for t in target_topics:
            if t not in topics:
                print(f"ã‚¨ãƒ©ãƒ¼: ä¸æ˜ãªãƒˆãƒ”ãƒƒã‚¯ '{t}'", file=sys.stderr)
                print(f"æœ‰åŠ¹ãªãƒˆãƒ”ãƒƒã‚¯: {', '.join(valid_topics)}, all", file=sys.stderr)
                sys.exit(1)

    report_paths = []
    json_paths = []
    collected_topics = []
    for tid in target_topics:
        md_file, json_file = collect_topic(tid, topics[tid], args.date)
        report_paths.append(md_file)
        json_paths.append(json_file)
        collected_topics.append(tid)

    # GitHub Actions ç”¨ã®å‡ºåŠ›
    github_output = os.environ.get("GITHUB_OUTPUT")
    if github_output:
        with open(github_output, "a") as f:
            f.write(f"report-dir={REPORTS_DIR}\n")
            f.write(f"topics-collected={','.join(collected_topics)}\n")
            f.write("report-paths<<EOF\n")
            for p in report_paths:
                f.write(f"{p}\n")
            f.write("EOF\n")
            f.write("report-json-paths<<EOF\n")
            for p in json_paths:
                f.write(f"{p}\n")
            f.write("EOF\n")


if __name__ == "__main__":
    main()
